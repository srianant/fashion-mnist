{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Acknowledgements:\n",
    "\n",
    "- Architecture: https://github.com/ravidziv/IDNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from mnist import MNIST\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.optimizers import adam\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import _pickle as cPickle\n",
    "import multiprocessing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from idnns.information import information_process  as inn\n",
    "from idnns.plots import plot_figures as plt_fig\n",
    "from idnns.networks.utils import data_shuffle\n",
    "NUM_CORES = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cov_net = 0\n",
    "calc_information = True\n",
    "run_in_parallel = False\n",
    "num_ephocs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "activation_function = 0\n",
    "interval_accuracy_display = 499\n",
    "save_grads = True\n",
    "num_of_repeats = 1\n",
    "calc_information_last = False\n",
    "num_of_bins = 30\n",
    "interval_information_display = 30\n",
    "save_ws = False\n",
    "inds = '[80]'\n",
    "num_of_samples = 400\n",
    "start_samples = 1\n",
    "num_classes = 10\n",
    "# The arch of the networks\n",
    "layers_sizes = [[10,7,5,4,3]]\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generic routine to plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_data(data, random=0):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        # display random images from data sample\n",
    "        if random:\n",
    "            idx = np.random.randint(0,data.shape[0])\n",
    "        else:\n",
    "            idx = i\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(data[idx].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Load train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load the data\n",
    "    name - the name of the dataset\n",
    "    random_labels - True if we want to return random labels to the dataset\n",
    "    return object with data and labels\"\"\"\n",
    "    print(\"Loading Fashion-MNIST_data...\")\n",
    "    C = type('type_C', (object,), {})\n",
    "    data_sets = C()\n",
    "    \n",
    "    data_sets_temp = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "    data_sets.data = np.concatenate((data_sets_temp.train.images, data_sets_temp.test.images), axis=0)\n",
    "    data_sets.labels = np.concatenate((data_sets_temp.train.labels, data_sets_temp.test.labels), axis=0)\n",
    "    print(\"Done loading...\")\n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Initialize arrays for information plane analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "# The percents of the train data samples\n",
    "inds = [map(int, inner.split(',')) for inner in re.findall(\"\\[(.*?)\\]\", inds)]        \n",
    "train_samples = np.linspace(1, 100, 199)[[[x * 2 - 2 for x in index] for index in inds]]\n",
    "# The indexs that we want to calculate the information for them in logspace interval\n",
    "epochs_indexes = np.unique(np.logspace(np.log2(start_samples), \n",
    "                                       np.log2(num_ephocs), \n",
    "                                       num_of_samples, dtype=int,base=2)) - 1\n",
    "max_size = np.max([len(_layers_size) for _layers_size in layers_sizes])\n",
    "\n",
    "# load data\n",
    "data_sets_org = load_data()\n",
    "hidden = []\n",
    "# create arrays for saving the data\n",
    "ws, grads, information, models, names, networks, weights = [\n",
    "    [[[[None] for k in range(len(train_samples))] for j in range(len(layers_sizes))]\n",
    "     for i in range(num_of_repeats)] for _ in range(7)]\n",
    "\n",
    "loss_train, loss_test, test_error, train_error, l1_norms, l2_norms = \\\n",
    "    [np.zeros((num_of_repeats, len(layers_sizes), len(train_samples), len(epochs_indexes)))\n",
    "     for _ in range(6)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Shuffle and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "indexes = epochs_indexes\n",
    "percent_of_train = train_samples\n",
    "data_sets = data_shuffle(data_sets_org, percent_of_train)\n",
    "\n",
    "# Normalize\n",
    "data_sets.train.data /= 255\n",
    "data_sets.test.data /= 255\n",
    "\n",
    "ws, estimted_label, gradients, infomration, models, weights = [[None] * len(indexes) for _ in range(6)]\n",
    "loss_func_test, loss_func_train, test_prediction, train_prediction = [np.zeros((len(indexes))) for _ in range(4)]\n",
    "input_size = data_sets_org.data.shape[1]\n",
    "num_of_classes = data_sets_org.labels.shape[1]\n",
    "layerSize = layers_sizes[0]\n",
    "all_layer_sizes = np.copy(layerSize)\n",
    "all_layer_sizes = np.insert(all_layer_sizes, 0, input_size)\n",
    "batch_size = np.min([batch_size, data_sets.train.data.shape[0]])\n",
    "batch_points = np.rint(np.arange(0, data_sets.train.data.shape[0] + 1, batch_size)).astype(dtype=np.int32)\n",
    "batch_points_test = np.rint(np.arange(0, data_sets.test.data.shape[0] + 1, batch_size)).astype(dtype=np.int32)\n",
    "batch_points_all = np.rint(np.arange(0, data_sets_org.data.shape[0] + 1, batch_size)).astype(dtype=np.int32)\n",
    "if data_sets_org.data.shape[0] not in batch_points_all:\n",
    "    batch_points_all = np.append(batch_points_all, [data_sets_org.data.shape[0]])\n",
    "if data_sets.train.data.shape[0] not in batch_points:\n",
    "    batch_points = np.append(batch_points, [data_sets.train.data.shape[0]])\n",
    "if data_sets.test.data.shape[0] not in batch_points_test:\n",
    "    batch_points_test = np.append(batch_points_test, [data_sets.test.data.shape[0]])\n",
    "\n",
    "# save copy of train/test data\n",
    "x_train = data_sets.train.data\n",
    "y_train = data_sets.train.labels\n",
    "x_test = data_sets.test.data\n",
    "y_test = data_sets.test.labels\n",
    "\n",
    "print(\"input_size:\", input_size)\n",
    "print(\"batch_size:\", batch_size)\n",
    "print(\"data_sets_org.data shape:\",data_sets_org.data.shape)\n",
    "print(\"data_sets_org.labels shape:\",data_sets_org.labels.shape)\n",
    "print(\"data_sets.train.data   (x_train)shape:\",data_sets.train.data.shape)\n",
    "print(\"data_sets.train.labels (y_train)shape:\",data_sets.train.labels.shape)\n",
    "print(\"data_sets.test.data    (x_test) shape:\",data_sets.test.data.shape)\n",
    "print(\"data_sets.test.labels  (y_test) shape:\",data_sets.test.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reshape (as required by keras models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # FC: Model-1\n",
    "image_size = 784 # 28 x 28\n",
    "x_train = x_train.reshape(x_train.shape[0], image_size) # Transform from matrix to vector\n",
    "x_test = x_test.reshape(x_test.shape[0], image_size) # Transform from matrix to vector\n",
    "input_shape = (img_rows, img_cols)\n",
    "\n",
    "# # CONVNET: Model-2\n",
    "# x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "# x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "# input_shape = (1, img_rows, img_cols)\n",
    "\n",
    "print(\"x_train: \",x_train.shape)\n",
    "print(\"x_test : \",x_test.shape)\n",
    "print(\"input_shape: \",input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plot train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_data(x_train, random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plot test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_data(x_test, random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-1: Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = Input(shape=[784])\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(10, activation='tanh')(input)\n",
    "x = Dense(7, activation='tanh')(x)\n",
    "x = Dense(5, activation='tanh')(x)\n",
    "x = Dense(4, activation='tanh')(x)\n",
    "x = Dense(3, activation='tanh')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=input, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-2 Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Model-2: val_acc: 92%\n",
    "# # WARNING: Takes in the order of several hours to run 100+ epochs\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size=(5, 5),\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build Information Plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tanh_activation(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax_activation(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def hidden_layer(model, x, hidden):\n",
    "    output = []\n",
    "    if len(hidden) == 0:\n",
    "        last_hidden = x;\n",
    "        for i in range(1, len(all_layer_sizes)+1):\n",
    "            w = model.layers[i].get_weights()[0]\n",
    "            b = model.layers[i].get_weights()[1]\n",
    "            input = np.matmul(last_hidden,w)+b\n",
    "            if (i == len(all_layer_sizes)):\n",
    "                last_hidden = softmax_activation(input)\n",
    "            else:\n",
    "                last_hidden = tanh_activation(input)\n",
    "            output.append(last_hidden)\n",
    "        hidden.append(output)        \n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def exctract_activity(batch_points_all, model, data_sets_org, hidden):\n",
    "    w_temp = []\n",
    "    for i in range(0, len(batch_points_all) - 1):\n",
    "        batch_xs = data_sets_org.data[batch_points_all[i]:batch_points_all[i + 1]]\n",
    "        batch_ys = data_sets_org.labels[batch_points_all[i]:batch_points_all[i + 1]]\n",
    "        w_temp_local = hidden_layer(model, batch_xs, hidden)\n",
    "        hidden = []\n",
    "        for s in range(len(w_temp_local[0])):\n",
    "            if i == 0:\n",
    "                w_temp.append(w_temp_local[0][s])\n",
    "            else:\n",
    "                w_temp[s] = np.concatenate((w_temp[s], w_temp_local[0][s]), axis=0)\n",
    "    return w_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_epoch_grads(model, batch_count):\n",
    "    weights = model.trainable_weights # weight tensors\n",
    "    weights = [weight for weight in weights] # filter down weights tensors to only ones which are trainable\n",
    "    _gradients = model.optimizer.get_gradients(model.total_loss, weights) # gradient tensors\n",
    "    \n",
    "    input_tensors = [model.inputs[0], # input data\n",
    "                     model.sample_weights[0], # how much to weight each sample by\n",
    "                     model.targets[0], # labels\n",
    "                     K.learning_phase(), # train or test mode\n",
    "    ]\n",
    "\n",
    "    get_gradients = K.function(inputs=input_tensors, outputs=_gradients)\n",
    "\n",
    "    start = batch_size*batch_count\n",
    "    end = start+batch_size\n",
    "    _batch_size = batch_size\n",
    "    if(start > x_train.shape[0]):\n",
    "        start = x_train.shape[0]\n",
    "        _batch_size = 32\n",
    "    if(end > x_train.shape[0]):\n",
    "        end = x_train.shape[0]\n",
    "        _batch_size = 32\n",
    "        \n",
    "    #print(\" size:\",start,\":\",end)\n",
    "    inputs = [x_train[start:end], # X\n",
    "              np.ones(_batch_size), # sample weights\n",
    "              y_train[start:end], # y\n",
    "              0 # learning phase in TEST mode\n",
    "    ]\n",
    "    \n",
    "    return get_gradients(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Callback class to visialize training progress\n",
    "class myCallbacks(keras.callbacks.History):\n",
    "    def __init__(self):\n",
    "        self.batch_count = 0\n",
    "        self.epoch_count = 0\n",
    "        self.k_count = 0\n",
    "        self.epochs_grads = []\n",
    "        self.hidden = []\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if self.epoch_count in indexes:\n",
    "            ws[self.k_count] = exctract_activity(batch_points_all, \n",
    "                                                 model, #self.model, \n",
    "                                                 data_sets_org, self.hidden)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        super(myCallbacks, self).on_epoch_end(epoch, logs)\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        axes = pd.DataFrame(self.history).plot()\n",
    "        axes.axvline(x=max((val_acc, i) for i, val_acc in enumerate(self.history['val_acc']))[1])\n",
    "        pyplot.show()\n",
    "        if self.epoch_count in indexes:\n",
    "            if save_grads:\n",
    "                gradients[self.k_count] = self.epochs_grads\n",
    "            self.k_count += 1\n",
    "        self.epochs_grads = []\n",
    "        self.epoch_count = self.epoch_count + 1\n",
    "        self.batch_count = 0\n",
    "        self.hidden = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if self.epoch_count in indexes:\n",
    "            epochs_grads_temp = get_epoch_grads(model, self.batch_count)\n",
    "            self.epochs_grads.append(epochs_grads_temp)\n",
    "        self.batch_count = self.batch_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    # Compile model with optimizer and loss function\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(x_train,y_train,\n",
    "          batch_size=batch_size, \n",
    "          epochs=num_ephocs,\n",
    "          verbose=True,\n",
    "          validation_data=(x_test, y_test),callbacks=[myCallbacks()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=pyplot.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    pyplot.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    pyplot.title(title)\n",
    "    pyplot.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    pyplot.xticks(tick_marks, classes, rotation=45)\n",
    "    pyplot.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        pyplot.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.ylabel('True label')\n",
    "    pyplot.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate_and_predict(model):\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    # predict y_hat\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    class_names = [\"T-shirt\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    pyplot.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Obtain INFORMATION PLANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_and_calc_inf_network(i, j, k, model):\n",
    "    print(\"train_and_calc:(i,j,k):\",i,j,k)\n",
    "    train_model(model)\n",
    "    evaluate_and_predict(model)\n",
    "    print(\"Done train network...\")\n",
    "    network = {}\n",
    "    network['ws'] = ws\n",
    "    network['test_prediction'] = test_prediction\n",
    "    network['train_prediction'] = train_prediction\n",
    "    network['loss_test'] = loss_func_test\n",
    "    network['loss_train'] = loss_func_train\n",
    "    network['gradients'] = gradients\n",
    "    network['model'] = model\n",
    "    network['information'] = []\n",
    "    print ('Calculating the infomration')\n",
    "    infomration = np.array([inn.get_information(network['ws'], data_sets_org.data, data_sets_org.labels,\n",
    "                                                num_of_bins, interval_information_display, network['model'],\n",
    "                                                layerSize,calc_parallel=False)])\n",
    "    network['information'] = infomration\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results = [train_and_calc_inf_network(i, j, k, model) \n",
    "           for i in range(len(train_samples)) \n",
    "           for j in range(len(layers_sizes)) \n",
    "           for k in range(num_of_repeats)]\n",
    "\n",
    "# Extract all the measures and orgainze it\n",
    "for i in range(len(train_samples)):\n",
    "    for j in range(len(layers_sizes)):\n",
    "        for k in range(num_of_repeats):\n",
    "            index = i * len(layers_sizes) * num_of_repeats + j * num_of_repeats + k\n",
    "            print(index)\n",
    "            current_network = results[index]\n",
    "            networks[k][j][i] = current_network\n",
    "            ws[k][j][i] = current_network['ws']\n",
    "            weights[k][j][i] = current_network['weights']\n",
    "            information[k][j][i] = current_network['information']\n",
    "            grads[k][i][i] = current_network['gradients']\n",
    "            test_error[k, j, i, :] = current_network['test_prediction']\n",
    "            train_error[k, j, i, :] = current_network['train_prediction']\n",
    "            loss_test[k, j, i, :] = current_network['loss_test']\n",
    "            loss_train[k, j, i, :] = current_network['loss_train']\n",
    "traind_network = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save Information Plane Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params = {'sampleLen': len(train_samples),\n",
    "          'nDistSmpls': 1,\n",
    "          'layerSizes': \",\".join(str(i) for i in layers_sizes[0]), 'nEpoch': num_ephocs, 'batch': batch_size,\n",
    "          'nRepeats': num_of_repeats, 'nEpochInds': len(epochs_indexes),\n",
    "          'LastEpochsInds': epochs_indexes[-1], 'DataName': 'var_u',\n",
    "          'lr': learning_rate}\n",
    "name = 'net'\n",
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "name_to_save = name + \"_\" + \"_\".join([str(i) + '=' + str(params[i]) for i in params])\n",
    "params['train_samples'], params['CPUs'], params[\n",
    "    'directory'], params['epochsInds'] = train_samples, NUM_CORES, name_to_save, epochs_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_data(parent_dir='jobs/', file_to_save='data.pickle'):\n",
    "    \"\"\"Save the data to the file \"\"\"\n",
    "    directory = '{0}/{1}{2}/'.format(os.getcwd(), parent_dir, params['directory'])\n",
    "\n",
    "    data = {'information': information,\n",
    "            'test_error': test_error, 'train_error': train_error, 'var_grad_val': grads,\n",
    "            'loss_test': loss_test, 'loss_train': loss_train, 'params': params, \n",
    "            'l1_norms': l1_norms, 'weights': weights, 'ws': ws}\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    dir_saved = directory\n",
    "    with open(dir_saved + file_to_save, 'wb') as f:\n",
    "        cPickle.dump(data, f, protocol=2)\n",
    "    return dir_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print ('Saving data')\n",
    "dir_saved = save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plot Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print ('Ploting figures')\n",
    "#Plot the newtork\n",
    "str_names = [[dir_saved]]\n",
    "mode = 2\n",
    "save_name = 'figure'\n",
    "plt_fig.plot_figures(str_names, mode, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
